{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c27d502-b9c7-43e3-b4cb-e1cac8630129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load API keys for OpenAI and HuggingFaceHub\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "534baad9-b7f6-4fc0-85b0-1d8ffbb00b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the documents we want to prompt an LLM about\n",
    "from langchain.document_loaders import TextLoader\n",
    "loader = TextLoader('./vector_db_documentation.txt')\n",
    "#loader = TextLoader('./State_of_the_union.txt')\n",
    "doc = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d86a36b5-ff4f-4de4-b650-08646c96f612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content=\"Algorithms Several algorithms can facilitate the creation of a vector index. Their common goal is to enable fast querying by creating a data structure that can be traversed quickly. They will commonly transform the representation of the original vector into a compressed form to optimize the query process.  However, as a user of Pinecone, you don't need to worry about the intricacies and selection of these various algorithms. Pinecone is designed to handle all the complexities and algorithmic\" metadata={'source': './vector_db_documentation.txt'}\n"
     ]
    }
   ],
   "source": [
    "#Chunk the documents into 500 character chunks using langchain's text splitter \"RucursiveCharacterTextSplitter\"\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap = 0\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(doc)\n",
    "print(chunks[0]) #split_documents produces a list of all the chunks created, printing out first chunk for example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65e13ac0-95c1-46fd-a783-c8391ee0376f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The chunks must now be embedded and stored in a vector store\n",
    "#Here we use HuggingFaceEmbeddings to embed the chunks, and FAISS (Facebook AI Similarity Search) as a vector store where the embedded vectors are stored\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "vec_db = FAISS.from_documents(chunks, embeddings) # vec_db is the vector store where the embeddings are stored\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13c6cee1-7f8d-4b30-82e5-19a6372a4a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='the emergence of vector databases as the computation engine that allows us to interact effectively with vector embeddings in our applications.  Vector databases are purpose-built databases that are specialized to tackle the problems that arise when managing vector embeddings in production scenarios. For that reason, they offer significant advantages over traditional scalar-based databases and standalone vector indexes.  In this post, we reviewed the key aspects of a vector database, including', metadata={'source': './vector_db_documentation.txt'}), Document(page_content=\"speed up the filtering tasks. Balancing the trade-offs between search performance and filtering accuracy is essential for providing efficient and relevant query results in vector databases.  Database Operations Unlike vector indexes, vector databases are equipped with a set of capabilities that makes them better qualified to be used in high scale production settings. Let's take a look at an overall overview of the components that are involved in operating the database.  Performance and Fault\", metadata={'source': './vector_db_documentation.txt'}), Document(page_content='which can later be used to populate new indexes.  API and SDKs This is where the rubber meets the road: Developers who interact with the database want to do so with an easy-to-use API, using a toolset that is familiar and comfortable. By providing a user-friendly interface, the vector database API layer simplifies the development of high-performance vector search applications.  In addition to the API, vector databases would often provide programming language specific SDKs that wrap the API. The', metadata={'source': './vector_db_documentation.txt'}), Document(page_content=\"activities within the vector database. This information is crucial for auditing purposes, and when security breaches happen, it helps trace back any unauthorized access or modifications.  Scalability and flexibility: As organizations grow and evolve, their access control needs may change. A robust access control system allows for seamless modification and expansion of user permissions, ensuring that data security remains intact throughout the organization's growth.   Backups and collections\", metadata={'source': './vector_db_documentation.txt'})]\n"
     ]
    }
   ],
   "source": [
    "#An example of using similarity search directly on FAISS vector store\n",
    "#The search uses Euclidean similarity search, which measures distance between two points in vector space. Measures how similar vectors are\n",
    "query = \"what are the strengths of a vector database\"\n",
    "#query = \"what did president obama say about the nations strengths?\"\n",
    "\n",
    "query_sim = vec_db.similarity_search(query) #query_sim holds results of the similarity search, the closest related chunks to the query.\n",
    "print(query_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06f60b94-6c0a-4d48-990c-4491fc74b723",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get requirements for question answer chain with LLM\n",
    "from langchain import HuggingFaceHub\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "#instantiate two llm models (OpenAI text-davinci-003, HuggingFaceHub google/flan-t5-xxl(designed for short answers)) \n",
    "llm_openai = OpenAI(model=\"text-davinci-003\", max_tokens=512)\n",
    "llm_flan = HuggingFaceHub(repo_id=\"google/flan-t5-xxl\", model_kwargs={\"temperature\":0.5, \"max_length\":512})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4d8bd50-bb42-4a27-bc91-7149ad6cbd22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mOpenAI\u001b[0m\n",
      "Params: {'model_name': 'text-davinci-003', 'temperature': 0.7, 'max_tokens': 512, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'request_timeout': None, 'logit_bias': {}}\n",
      "\u001b[1mHuggingFaceHub\u001b[0m\n",
      "Params: {'repo_id': 'google/flan-t5-xxl', 'task': None, 'model_kwargs': {'temperature': 0.5, 'max_length': 512}}\n"
     ]
    }
   ],
   "source": [
    "print(llm_openai)\n",
    "print(llm_flan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99d25451-b550-4fdc-af2f-80ca477d7270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI Response: \n",
      " Vector databases are specialized to tackle the problems that arise when managing vector embeddings in production scenarios, and offer significant advantages over traditional scalar-based databases and standalone vector indexes. They are equipped with a set of capabilities that makes them better qualified to be used in high scale production settings, such as efficient search performance, relevance of query results, and user-friendly API and SDKs. \n",
      "\n",
      "HuggingFaceHub Response: \n",
      "they offer significant advantages over traditional scalar-based databases and standalone vector indexes\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "#create the chain for each model using langchain load_qa_chain\n",
    "chain_openAI = load_qa_chain(llm_openai, chain_type=\"stuff\")\n",
    "chain_HuggingFaceHub = load_qa_chain(llm_flan, chain_type=\"stuff\")\n",
    "\n",
    "#example prompts\n",
    "#query = \"what year of presidency is this for Obama?\"\n",
    "#query = \"what did president obama say about the nations strengths?\"\n",
    "#query = \"what are the four big questions the country needs to answer?\"\n",
    "\n",
    "#query = \"what is a vector database?\"\n",
    "query = \"what are the strengths of a vector database?\"\n",
    "#query = \"when should I use a vector store?\"\n",
    "\n",
    "query_sim = vec_db.similarity_search(query) #Gather the most related chunks to the query\n",
    "\n",
    "#run the chain on the query and the related chunks from the documentation\n",
    "print(\"OpenAI Response: \")\n",
    "print(chain_openAI.run(input_documents=query_sim, question=query),'\\n')\n",
    "print(\"HuggingFaceHub Response: \")\n",
    "print(chain_HuggingFaceHub.run(input_documents=query_sim, question=query))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
